
import os
import subprocess

class OllamaError(RuntimeError):
    pass


def _stub_response(context: str, question: str) -> str:
    # Make a short, useful stub that includes the question and a preview of the context.
    preview = (context or "").strip().replace("\n", " ")
    if len(preview) > 300:
        preview = preview[:300].rsplit(" ", 1)[0] + "..."
    return (
        "STUB ANSWER: This response was generated by a local development stub because no local LLM is available.\n\n"
        f"Question: {question}\n\nContext preview: {preview}"
    )


def generate(context: str, question: str, system: str, model: str = "mistral"):
    """Generate text using a local Ollama CLI, but fall back to a safe stub for dev.

    By default this function will return a stub so you can test the full RAG flow
    without installing Ollama. To enable real Ollama execution, set the
    environment variable `OLLAMA_STUB` to "0" and ensure `ollama` is on PATH.
    """
    use_stub = os.getenv("OLLAMA_STUB", "1") != "0"
    if use_stub:
        return _stub_response(context, question)

    # If not stubbing, attempt to call the Ollama CLI
    prompt = f"{system}\n\nContext:\n{context}\n\nQuestion:\n{question}\n"
    try:
        res = subprocess.run(["ollama", "run", model],
                             input=prompt.encode(), capture_output=True, check=False)
    except FileNotFoundError as e:
        raise OllamaError("`ollama` CLI not found. Install Ollama or set OLLAMA_STUB=1 to use the dev stub.") from e

    stdout = res.stdout.decode().strip()
    stderr = res.stderr.decode().strip()
    if res.returncode != 0:
        msg = f"Ollama exited with code {res.returncode}. {stderr or 'No stderr.'}"
        raise OllamaError(msg)

    if not stdout:
        # No output but exit code 0 â€” treat as error to be explicit
        raise OllamaError("Ollama returned no output.")

    return stdout
